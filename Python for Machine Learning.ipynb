{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Machine learning\n",
    "\n",
    "The basic idea of any machine learning model is to have a large number of inputs and the outputs. After analyzing more and more data, it tries to figure out the relationship between input and output.\n",
    "\n",
    "Consider a very simple example when you have to decide whether to wear a jacket or not based on the outside temperature. You have the data below and we call it training data. \n",
    "\n",
    "\n",
    "| Outside Temperature | Wear a Jacket |\n",
    "|---------------------|---------------|\n",
    "| 90°F                | No            |\n",
    "| 80°F                | No            |\n",
    "| 70°F                | No            |\n",
    "| 60°F                | Yes           |\n",
    "| 20°F                | Yes           |\n",
    "\n",
    "Somehow, we find out a connection between the input (temperature) and the output (decision to wear a jacket).\n",
    "If the temperature is 65°F, you would still wear a jacket although you were never told the outcome for that particular temperature.\n",
    "\n",
    "Now, let's move on to a simple regression problem which the computer will solve for us.\n",
    "Before we begin, we need to import the scikit-learn package, it provides easy to use functions and a lot of machine learning models. We will use it for today's workshop. \n",
    "\n",
    "```python\n",
    "# install scikit package using conda\n",
    "conda install -c conda-forge scikit-image\n",
    "# using pip\n",
    "pip install -U scikit-learn\n",
    "# install yellowbrick package using conda\n",
    "conda install -c districtdatalabs yellowbrick\n",
    "# using pip\n",
    "pip install yellowbrick\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import yellowbrick\n",
    "print(\"sklearn version is:\" + sklearn.__version__)\n",
    "print(\"yellowbrick version is:\" + yellowbrick.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Training Set\n",
    "Here, X is the input and y is the output.\n",
    "\n",
    "| x1 | x2 | y  |\n",
    "|----|----|----|\n",
    "| 1  | 2  | 5  |\n",
    "| 4  | 5  | 14 |\n",
    "| 11 | 12 | 35 |\n",
    "| 21 | 22 | 65 |\n",
    "| 5  | 5  | 15 |\n",
    "\n",
    "Given the training set you could easily guess that the output (y) is (x1 + 2````*````x2 ).\n",
    "\n",
    "## How to Generate a Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import randint function from random package\n",
    "from random import randint\n",
    "\n",
    "# Create two empty list to storage training input and output data \n",
    "TrainInput = list()\n",
    "TrainOutput = list()\n",
    "\n",
    "# Generate 100 random set of x1 and x2\n",
    "for i in range(100):\n",
    "    x1 = randint(0, 1000) # generate random integers between 0 and 1000. \n",
    "    x2 = randint(0, 1000)\n",
    "    y = x1 + (2*x2)\n",
    "    #append method is to add x1 and x2 to the Train list.\n",
    "    TrainInput.append([x1, x2]) \n",
    "    TrainOutput.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainInput[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainOutput[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Model: Linear Regression\n",
    "Working with linear regression model is simple. Create a model, train it and then test it.\n",
    "\n",
    "### Train the Model\n",
    "We have generated the training data already, so create a linear regression model and pass it the training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "predictor=LinearRegression()\n",
    "# fit the model using traininput and trainoutput\n",
    "predictor.fit(X=TrainInput, y=TrainOutput)\n",
    "\n",
    "# get the coefficients and print out\n",
    "coefficient=predictor.coef_\n",
    "print('Coefficient : {}.'.format(coefficient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data\n",
    "\n",
    "X = [[10, 20]]\n",
    "\n",
    "The outcome should be 10+2```*```20 =50. Let us see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = [[10, 20]]\n",
    "Outcome = predictor.predict(X=Xtest)\n",
    "print('Outcome: {}'.format(Outcome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Linear Regression Example\n",
    "\n",
    "Sales (in thousands of units) for a particular product as a function of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media. Suppose that in our role as statistical consultants we are asked to suggest.\n",
    "1. We want to find a function that given input budgets for TV, radio and newspaper predicts the output sales.\n",
    "2. Which media contribute to sales?\n",
    "3. Visualize the relationship between the features and response. Note: In machine learning variables are usually called features.\n",
    "\n",
    "Reference: https://medium.com/simple-ai/linear-regression-intro-to-machine-learning-6-6e320dbdaf06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the packages\n",
    "import pandas as pd\n",
    "#import the adversting.csv data\n",
    "data =  pd.read_csv(\"Advertising.csv\")\n",
    "# view the first five rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python list of feature names\n",
    "feature_names = ['TV', 'radio', 'newspaper']\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the list to select a subset of the original DataFrame\n",
    "X = data[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the list to select a subeset of the original DataFrame\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize relationship between the variables\n",
    "from yellowbrick.features import Rank2D\n",
    "visualizer = Rank2D(algorithm='pearson')\n",
    "visualizer.fit(data)\n",
    "visualizer.transform(data)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split X and y into traning and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) # random_state=int, random_state is the seed used by the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Linear Regression Model\n",
    "linreg = LinearRegression()\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "# Check the coefficient\n",
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the regression line use seaborn \n",
    "# compute the R square value\n",
    "from sklearn.metrics import r2_score\n",
    "print('R2 Score:', r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "# Visualzie the training and fitting model and the residual histogram\n",
    "visualizer = ResidualsPlot(linreg)\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof()                 # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1:\n",
    "1. Import the packages using the python code below:\n",
    "```Python\n",
    "#import the packages\n",
    "import pandas as pd\n",
    "#import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from yellowbrick.features import Rank2D\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "```\n",
    "2. Import the data named \"'bikeshare.csv\" as a dataframe.\n",
    "\n",
    "```python\n",
    "data =  pd.read_csv(\"bikeshare.csv\")\n",
    "```\n",
    "3. Create a Python list of feature names:\n",
    "    1. X is \"season\", \"month\", \"hour\", \"holiday\", \"weekday\", \"workingday\",\n",
    "    \"weather\", \"temp\", \"feelslike\", \"humidity\", \"windspeed\".\n",
    "4. Use the list to select a subset, X, of the original DataFrame \n",
    "\n",
    "5. Use the python commands below to show the pearson correlation matrix\n",
    "\n",
    "```python\n",
    "# visualize relationship between the variables\n",
    "from yellowbrick.features import Rank2D\n",
    "visualizer = Rank2D(algorithm='pearson')\n",
    "visualizer.fit(X, y)\n",
    "visualizer.transform(X)\n",
    "visualizer.show()\n",
    "```\n",
    "\n",
    "6. Use the list to select a subset, y, of the original DataFrame \n",
    "    * Y is \"riders\"\n",
    "    \n",
    "7. Split X and y into training and testing sets  \n",
    "8. Create a linear regression function and fit the model from the training data, and test using the testing data.\n",
    "9. Check the coefficient\n",
    "10. Visualize the residuals of training and testing model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the packages\n",
    "import pandas as pd\n",
    "#import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from yellowbrick.features import Rank2D\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "# import data\n",
    "BikeData =  pd.read_csv(r\"I:\\Classes\\OIT_Training\\Python for Machine Learning\\xzhu8\\bikeshare.csv\")\n",
    "print(BikeData.head())\n",
    "featureNames = [\"season\", \"month\", \"hour\", \"holiday\", \"weekday\", \"workingday\", \"weather\", \"temp\", \"feelslike\", \"humidity\", \"windspeed\"]\n",
    "X = BikeData[featureNames]\n",
    "y = BikeData['riders']\n",
    "\n",
    "# visualize relationship between the variables\n",
    "from yellowbrick.features import Rank2D\n",
    "visualizer = Rank2D(algorithm='pearson')\n",
    "visualizer.fit(X, y)\n",
    "visualizer.transform(X)\n",
    "visualizer.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Classification Problem\n",
    "\n",
    "**Supervised learning:** you train the machine using data which is well \"labeled.\" It means some data is already tagged with the correct answer. It can be compared to learning which takes place in the presence of a supervisor or a teacher. \n",
    "\n",
    "**Unsupervised learning:** a machine learning technique, where you do not need to supervise the model. Instead, you need to allow the model to work on its own to discover information. It mainly deals with the unlabelled data.\n",
    "\n",
    "Supervised learning classified into two categories of algorithms:\n",
    "\n",
    "**Regression:** A regression problem is when the output variable is a real value, such as “dollars” or “weight”.\n",
    "\n",
    "**Classification:** A classification problem is when the output variable is a category, such as “red” or “blue” or “disease” and “no disease”.\n",
    "\n",
    "For instance, suppose you are given a basket filled with different kinds of fruits and ask you to train the model and then to predict the fruit type using test data. \n",
    "The fruits dataset was created by Dr. Iain Murray from University of Edinburgh. He bought a few dozen oranges, mandarin, lemons and apples of different varieties, and recorded their measurements in a table. And then the professors at University of Michigan formatted the fruits data slightly. Let us import the data and see the first several rows of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fruits = pd.read_table(\"fruit_data_with_colors.txt\")\n",
    "fruits.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the dataset represents one piece of the fruit as represented by several features that are in the table’s columns.\n",
    "\n",
    "We have 59 pieces of fruits and 7 features in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fruits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four types of fruits in the dataset: apple, mandarin, orange, and lemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits.groupby('fruit_name').size()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(fruits['fruit_name'], color='b', label='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot \n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=\"width\", y=\"height\",hue=\"fruit_name\", style=\"fruit_name\", data=fruits)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics \n",
    "fruits.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram for each numeric imput variable\n",
    "from scipy.stats import norm\n",
    "fruits.drop('fruit_label', axis=1).hist(bins=30, figsize=(9,9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some numerical values do not have the same scale, so we need to scale them. We will split the dataset into training and test sets first. Then we scale the training data and then apply scaling to the test set, because in practice you are not provided with test data and you just have to evaluate your model on test data.\n",
    "\n",
    "Here we use MinMaxScaler, which rescales the data set such that all feature values are in the range [0, 1]. \n",
    "\n",
    "The transformation formula is given by:\n",
    "\n",
    "$ z_i = \\frac{x_i - min(x_i)}{max(x_i) - min(x_i)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a feature list and y\n",
    "feature_names = ['mass', 'width', 'height', 'color_score']\n",
    "X = fruits[feature_names]\n",
    "y = fruits['fruit_label']\n",
    "#split the dataset into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "#Use minmax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a logistic regression model\n",
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
    "\n",
    "Reference: https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "\n",
    "**Confusion matrix:** a table to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
    "\n",
    "**Accuracy:** the ratio of number of correct predictions to the total number of input samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "logreg = LogisticRegression()\n",
    "#logreg = LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(logreg)\n",
    "# Fit the passed model. \n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "# print out the confusion matrix\n",
    "cm.show()\n",
    "\n",
    "# :.f Here we specify 2 digits of precision and f is used to represent floating point number.\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(logreg.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(logreg.score(X_test, y_test))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree \n",
    "A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n",
    "Reference:https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dct = DecisionTreeClassifier()\n",
    "cm = ConfusionMatrix(dct)\n",
    "\n",
    "# Fit the passed model. \n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.show()\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(dct.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(dct.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n",
    "Reference:https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "cm = ConfusionMatrix(knn)\n",
    "\n",
    "# Fit the passed model. \n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.show()\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "Linear Discriminant Analysis is a dimensionality reduction technique used as a preprocessing step in Machine Learning and pattern classification applications.\n",
    "The main goal of dimensionality reduction techniques is to reduce the dimensions by removing the redundant and dependent features by transforming the features from higher dimensional space to a space with lower dimensions.\n",
    "Reference:https://medium.com/@srishtisawla/linear-discriminant-analysis-d38decf48105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "cm = ConfusionMatrix(lda)\n",
    "\n",
    "# Fit the passed model. \n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.show()\n",
    "print('Accuracy of LDA classifier on training set: {:.2f}'\n",
    "     .format(lda.score(X_train, y_train)))\n",
    "print('Accuracy of LDA classifier on test set: {:.2f}'\n",
    "     .format(lda.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "**Pros:** It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n",
    "When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
    "It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
    "\n",
    "**Cons:** If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\n",
    "On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "cm = ConfusionMatrix(gnb)\n",
    "\n",
    "# Fit the passed model. \n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.show()\n",
    "print('Accuracy of GNB classifier on training set: {:.2f}'\n",
    "     .format(gnb.score(X_train, y_train)))\n",
    "print('Accuracy of GNB classifier on test set: {:.2f}'\n",
    "     .format(gnb.score(X_test, y_test)))\n",
    "# get the predict probablity;\n",
    "print(cm.predict_proba(X_test)[0:2])\n",
    "print(cm.predict(X_test)[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "Keywords: bootstrap aggregation or bagging\n",
    "          ensemble model, voting\n",
    "\n",
    "**RF Algorithm:**\n",
    "Fits a bunch of trees on \"random samples\n",
    "from our sample\" (called bootstrap samples)\n",
    "& they all vote on best class. The votes \n",
    "aggregated to choose the winning class.\n",
    "The concept of combining predictions \n",
    "like this is called \"bagging\" or \n",
    "Bootstrap Aggregation.\n",
    "\n",
    "Details:\n",
    "\n",
    "1. Randomly select (with replacement) both\n",
    "   N observations and a subset of predictors\n",
    "   to create 100-500 subsets of data.\n",
    "\n",
    "2. Fit a \"bushy\" tree to each sample\n",
    "   e.g. no pruning so each WILL overfit!\n",
    "\n",
    "3. At each split, variables are randomly sampled\n",
    "\n",
    "4. Have each model make a prediction, then \n",
    "   count (when classifying) or average \n",
    "   (when regressing) their predictions\n",
    "   (weights may be used based on model accuracy)\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "cm = ConfusionMatrix(rfc)\n",
    "\n",
    "# Fit the passed model. \n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.show()\n",
    "print('Accuracy of RFC classifier on training set: {:.2f}'\n",
    "     .format(rfc.score(X_train, y_train)))\n",
    "print('Accuracy of RFC classifier on test set: {:.2f}'\n",
    "     .format(rfc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks (NN)\n",
    "\n",
    "**Neural Network Algorithm**\n",
    "\n",
    "* Simulates human brain by weighting\n",
    "  \"neurons\" stored in \"hidden layers\"\n",
    "  \n",
    "* Each training observation adjusts\n",
    "  the impact of each neuron, often\n",
    "  through \"back-propogation\"\n",
    "\n",
    "* Deep Learning uses many layers\n",
    "  and many neurons per layer\n",
    "  \n",
    "\n",
    "**Pros:**  1) Excellent results for extreme \n",
    "  complexity e.g. voice, \n",
    "  image recognition 2) Though the model consists of a set\n",
    "  of equations that is fairly small\n",
    "  compared to many other methods 3) Model is quick to apply to new data\n",
    "\n",
    "    \n",
    "**Cons:** 1) Computationally intensive\n",
    "  to train the model 2) Performance on numerical data\n",
    "  rarely much better than faster\n",
    "  methods e.g. rf, gbm 3) Models are impossible to interpret 4) Extremely sensitive to multicollinearity\n",
    "  (use PCA first or method = \"pcaNNet\")\n",
    "\n",
    "\n",
    "**Neural Network Tuning Parameters**\n",
    "\n",
    "* Depends on specific type but\n",
    "  in general:\n",
    "  \n",
    "* Number of hidden layers.\n",
    "\n",
    "* Number of neurons per layer\n",
    "\n",
    "* Type of feedback mechanism\n",
    "  e.g. back-propogation\n",
    "  \n",
    "  Reference:  https://www.heatonresearch.com/2017/06/01/hidden-layers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter=500)  # max_iter: Maximum number of iterations, default is 200.\n",
    "# hidden_layer_sizes (20, 20, 20): is a three-layer hidden layer NN with 20 neurons in the ith hidden layer.\n",
    "cm = ConfusionMatrix(mlp)\n",
    "\n",
    "# Fit fits the passed model. \n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.show()\n",
    "print('Accuracy of MLP classifier on training set: {:.2f}'\n",
    "     .format(mlp.score(X_train, y_train)))\n",
    "print('Accuracy of MLP classifier on test set: {:.2f}'\n",
    "     .format(mlp.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Common metrics for evaluating classifiers:\n",
    "\n",
    "![image.png](confusion.png)\n",
    "\n",
    "\n",
    "**Precision** is the number of correct positive results divided by the number of all predicted positive results .\n",
    "\n",
    "**Recall** is the number of correct positive results divided by the number of actual positive results that should have been returned.\n",
    "\n",
    "The **F1** score is a measure of a test’s accuracy. It considers both the precision and the recall of the test to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.\n",
    "```python\n",
    "precision = true positives / (true positives + false positives)\n",
    "\n",
    "recall = true positives / (false negatives + true positives)\n",
    "\n",
    "F1 score = 2 * (true positives) / (2 * true positives + false positives + false negatives)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassificationReport(knn, support=True)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted vs Actual in Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new=knn.predict(X_test)\n",
    "test = list(zip(y_new, y_test))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise2: \n",
    "\n",
    "The dataset we will be working with in this tutorial is the Breast Cancer Wisconsin Diagnostic Database. The dataset includes various information about breast cancer tumors, as well as classification labels of malignant or benign. The dataset has 569 instances, or data, on 569 tumors and includes information on 30 attributes, or features, such as the radius of the tumor, texture, smoothness, and area.\n",
    "\n",
    "Using this dataset, we will build a machine learning model to use tumor information to predict whether or not a tumor is malignant or benign.\n",
    "\n",
    "The syntax below imported and loaded the dataset, splitted the data into training and test and also applied Gaussian Naive Bayes model to the data.\n",
    "You need to fit the Logistic Regression and Random Forest model and find the best one. \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "# Organize our data\n",
    "y_names = data['target_names']\n",
    "y = data['target']\n",
    "feature_names = data['feature_names']\n",
    "X = data['data']\n",
    "# Split our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "#fit Gaussian Naive model\n",
    "gnb = GaussianNB()\n",
    "cm = ConfusionMatrix(gnb, classes=y_names, label_encoder={0:'malignant', 1: 'benign'}  )\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "# How did we do?\n",
    "cm.show()\n",
    "print('Accuracy of RFC classifier on training set: {:.2f}'\n",
    "     .format(gnb.score(X_train, y_train)))\n",
    "print('Accuracy of RFC classifier on test set: {:.2f}'\n",
    "     .format(gnb.score(X_test, y_test)))\n",
    "     \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "# check the data\n",
    "data['data'][0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize our data\n",
    "y_names = data['target_names']\n",
    "y = data['target']\n",
    "feature_names = data['feature_names']\n",
    "X = data['data']\n",
    "\n",
    "\n",
    "# Split our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "\n",
    "#fit Gaussian Naive model\n",
    "gnb = GaussianNB()\n",
    "cm = ConfusionMatrix(gnb, classes=y_names, label_encoder={0:'malignant', 1: 'benign'}  )\n",
    "# Fit fits the passed model. \n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.show()\n",
    "print('Accuracy of GNB classifier on training set: {:.2f}'\n",
    "     .format(gnb.score(X_train, y_train)))\n",
    "print('Accuracy of GNB classifier on test set: {:.2f}'\n",
    "     .format(gnb.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "When dealing with real-world problems, most of the time, data will not come with predefined labels, so we will want to develop machine learning models that can correctly classified the data by finding some commomality in the features to predict the classes on new data.\n",
    "\n",
    "Two main types of problems in unsupervised learning:\n",
    "\n",
    "* Clustering\n",
    "* Dimension Reduction\n",
    "\n",
    "This workshop we will cover the clustering problems. \n",
    "\n",
    "## Clustering Analysis\n",
    "\n",
    "Cluster analysis or clustering is to group a set of objects that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).\n",
    "![image.png](Unsupervise3.png)\n",
    "\n",
    "Clustering, however, has many different names (with respect to the fields it is being applied):\n",
    "\n",
    "* Cluster analysis\n",
    "* Automatic classification\n",
    "* Data segmentation\n",
    "\n",
    "**All the above names essentially mean clustering.**\n",
    "\n",
    "Cluster analysis have an incredible wide range of applications and are quite useful to solve real world problems such as anomaly detection, recommending systems, documents grouping, or finding customers with common interests based on their purchases.\n",
    "Some of the most common clustering algorithms will be explored in the workshop, are:\n",
    "* K-Means\n",
    "* Hierarchical Clustering\n",
    "* Density Based Scan Clustering (DBSCAN)\n",
    "* Gaussian Clustering Model\n",
    "\n",
    "## Choosing a Problem\n",
    "\n",
    "We will take an example of market segmentation. The data set contains 30 samples and two features, satisfaction and loyalty. We will try to analyse the type of customers in the market based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "# import data\n",
    "data=pd.read_csv(\"kmeans clustering.csv\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=\"Satisfaction\", y=\"Loyalty\",  data=data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## K-means Cluster\n",
    "\n",
    "k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster (Definition from Wiki).\n",
    "\n",
    "### How the K-means algorithm works\n",
    "To process the learning data, the K-means algorithm in data mining starts with a first group of randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative (repetitive) calculations to optimize the positions of the centroids\n",
    "It halts creating and optimizing clusters when either:\n",
    "The centroids have stabilized — there is no change in their values because the clustering has been successful.\n",
    "The defined number of iterations has been achieved.\n",
    "\n",
    "reference: https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# copy the data and ignore the feature names and store the data into a variable X.\n",
    "x = data.copy()\n",
    "\n",
    "# create a variable kmeans using kmeans function and passing the argument 2 in the Kmeans\n",
    "kmeans = KMeans(2) # split the customers into two clusters\n",
    "kmeans.fit(x)\n",
    "\n",
    "# Clustering result\n",
    "clusters = x.copy()\n",
    "clusters['cluster_pred'] = kmeans.fit_predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(clusters['Satisfaction'], clusters['Loyalty'],c=clusters['cluster_pred'], cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalty')\n",
    "plt.show()\n",
    "# colormap reference: https://matplotlib.org/3.1.1/gallery/color/colormap_reference.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "The biggest problem here is that Satisfaction is chosen as a feature and loyalty has been ignored. \n",
    "Satisfaction was chosen as the feature because it had large values.\n",
    "So the problem is because both variables are not scaled. First we have to standardize the data to make both two variables have equal weights in our clustering. We will scale the data around zero mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "x_scaled = preprocessing.scale(x)\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Elbow Method:\n",
    "Here the elbow method can help us figure out how many clusters we need. What elbow method does is it starts of with making one cluster to the number of clusters in our sample and with the kmeans inertia value. Based on the values, we determine what the appropriate number of clusters is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range (1, 30):\n",
    "    kmeans = KMeans(i)\n",
    "    kmeans.fit(x_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "# visualized the Elbow method\n",
    "plt.plot(range(1,30), wcss)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the elbow point comes at around 4, so the optimal number of cluster is 4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_new = KMeans(4)\n",
    "kmeans.fit(x_scaled)\n",
    "cluster_new = x.copy()\n",
    "cluster_new['cluster_pred']=kmeans_new.fit_predict(x_scaled)\n",
    "#cluster_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the newly cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(clusters['Satisfaction'], clusters['Loyalty'],c=cluster_new['cluster_pred'], cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalty')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "### How Hierarchical Clustering Works\n",
    "Hierarchical clustering starts by treating each observation as a separate cluster. Then, it repeatedly executes the following two steps: (1) identify the two clusters that are closest together, and (2) merge the two most similar clusters. This continues until all the clusters are merged together. This is illustrated in the diagrams below.\n",
    "\n",
    "### Dendrogram\n",
    "A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters.\n",
    "\n",
    "The key to interpreting a dendrogram is to focus on the height at which any two objects are joined together. In the example below, we can see that 4 and 22 are most similar, as the height of the link that joins them together is the smallest.\n",
    "\n",
    "Observations are allocated to clusters by drawing a horizontal line through the dendrogram. Observations that are joined together below the line are in clusters.\n",
    "\n",
    "Reference: https://www.displayr.com/what-is-dendrogram/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dendrogram \n",
    "from scipy.cluster import hierarchy\n",
    "Z = hierarchy.linkage(x_scaled,'ward')\n",
    "dn = hierarchy.dendrogram(Z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hierarchical clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage = 'ward')\n",
    "\n",
    "# save clusters for chart\n",
    "cluster_new['cluster_hc'] = hc.fit_predict(x_scaled)\n",
    "#cluster_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cluster\n",
    "plt.close()\n",
    "plt.scatter(cluster_new['Satisfaction'], cluster_new['Loyalty'],c=cluster_new['cluster_hc'], cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalty')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Based Scan Clustering (DBSCAN)\n",
    "\n",
    "The key idea is that for each point of a cluster, the neighborhood of a given radius has to contain at least a minimum number of points. Compared to centroid-based clustering like K-Means, density-based clustering works by identifying “dense” clusters of points, allowing it to learn clusters of arbitrary shape and identify outliers in the data. \n",
    "\n",
    "**DBSCAN algorithm requires two parameters:**\n",
    "\n",
    "**eps** : It defines the neighborhood around a data point i.e. if the distance between two points is lower or equal to ‘eps’ then they are considered as neighbors. If the eps value is chosen too small then large part of the data will be considered as outliers. If it is chosen very large then the clusters will merge and majority of the data points will be in the same clusters. One way to find the eps value is based on the k-distance graph.\n",
    "\n",
    "**MinPts**: Minimum number of neighbors (data points) within eps radius. Larger the dataset, the larger value of MinPts must be chosen. As a general rule, the minimum MinPts can be derived from the number of dimensions D in the dataset as, MinPts >= D+1. The minimum value of MinPts must be chosen at least 3.\n",
    "In this algorithm, we have 3 types of data points.\n",
    "\n",
    "**Core Point**: A point is a core point if it has more than MinPts points within eps.\n",
    "**Border Point**: A point which has fewer than MinPts within eps but it is in the neighborhood of a core point.\n",
    "**Noise or outlier**: A point which is not a core point or border point.\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "db_default = DBSCAN(eps = 0.6, min_samples = 3)\n",
    "\n",
    "# save clusters for chart\n",
    "cluster_new['cluster_db'] =db_default.fit_predict(x_scaled)\n",
    "#print(cluster_db)\n",
    "\n",
    "# plot the cluster\n",
    "plt.close()\n",
    "plt.scatter(cluster_new['Satisfaction'], cluster_new['Loyalty'],c=cluster_new['cluster_db'], cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalty')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.\n",
    "\n",
    "The GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data.\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/mixture.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=4) # four clusters.\n",
    "\n",
    "# save cluster into a column\n",
    "cluster_new['cluster_gmm'] = gmm.fit_predict(x_scaled)\n",
    "\n",
    "# plot the cluter\n",
    "plt.close()\n",
    "plt.scatter(cluster_new['Satisfaction'], cluster_new['Loyalty'],c = cluster_new['cluster_gmm'], cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalty')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the Kmean cluster again.\n",
    "plt.scatter(clusters['Satisfaction'], clusters['Loyalty'],c=cluster_new['cluster_pred'], cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalthy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis (The final step):\n",
    "Through the given figure following things can be interpreted:\n",
    "\n",
    "The yellow are the people who are less satisfied and less loyal and therefore can be termed as alienated.\n",
    "The purple dots are people with high loyalty and less satisfaction.\n",
    "The blue dots are the people with high loyalty and high satisfaction and they are the fans.\n",
    "The red dots are the people who are in the midst of things.\n",
    "The ultimate goal of any businessman would be to have as many people up there in the fans category. We are ready with a solution and we can target the audience as per our analysis. For example, the crowd who are supporters can easily be turned into fans by fulfilling their satisfaction level.\n",
    "\n",
    "reference:\n",
    "1. https://medium.com/code-to-express/k-means-clustering-for-beginners-using-python-from-scratch-f20e79c8ad00\n",
    "2. https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n",
    "3. https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html#spectral-clustering\n",
    "\n",
    "## Another example\n",
    "The python commands below generates 1000 random dataset of X and Y and the scatter plot of X can make two moons shape. We are going to use different linkage methods for hierarchical clustering on datasets that are “interesting”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# generate 2d classification dataset\n",
    "X,y= make_moons(n_samples=1000, noise=0.05)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "\n",
    "# Scatter plot of the moon data.\n",
    "plt.scatter(df['x'], df['y'], c=y, cmap = 'rainbow')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heirotical ward method\n",
    "x = df[['x','y']]\n",
    "\n",
    "# Create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=2, affinity = 'euclidean', linkage = 'ward')\n",
    "\n",
    "# save clusters for chart\n",
    "cluster_moon = df.copy()\n",
    "\n",
    "# create a variable named cluster_ward  in cluster_moon data.\n",
    "cluster_moon['cluster_ward'] = hc.fit_predict(x)\n",
    "\n",
    "# print out the data.\n",
    "cluster_moon.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the output\n",
    "plt.scatter(cluster_moon['x'], cluster_moon['y'],c=cluster_moon['cluster_ward'], cmap = 'rainbow')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Used the data above and try the single and complete linkage clustering method and plot the data.\n",
    "\n",
    "```python\n",
    "#heirotical single method\n",
    "# Create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=2, affinity = 'euclidean', linkage = 'single')\n",
    "#heirotical complete method\n",
    "\n",
    "# Create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=2, affinity = 'euclidean', linkage = 'complete')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "Exercise 1\n",
    "1. Import the packages using the python code below:\n",
    "```Python\n",
    "#import the packages\n",
    "import pandas as pd\n",
    "#import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from yellowbrick.features import Rank2D\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "```\n",
    "2. Import the data named \"'bikeshare.csv\" as a dataframe.\n",
    "```python\n",
    "data =  pd.read_csv(r\"C:\\Users\\XZHU8\\Documents\\OIT related\\Workshop\\Python for Machine Learning\\bikeshare.csv\")\n",
    "```\n",
    "3. Create a Python list of feature names:\n",
    "    X is \"season\", \"month\", \"hour\", \"holiday\", \"weekday\", \"workingday\",\n",
    "    \"weather\", \"temp\", \"feelslike\", \"humidity\", \"windspeed\".\n",
    "    \n",
    "4. Use the list to select a subset, X, of the original DataFrame \n",
    "\n",
    "5. Use the python syntax below to show the pearson correlaion matrix\n",
    "\n",
    "6. Use the list to select a subset, y, of the original DataFrame: \n",
    "    Y is \"riders\"\n",
    "\n",
    "7. Split X and y into training and testing sets \n",
    "\n",
    "8. Create a linear regression function and fit the model from the training data, and test using the testing data.\n",
    "\n",
    "9. Check the coeffiecient.\n",
    "\n",
    "10. Visulize the residuals of training and testing model \n",
    "\n",
    "```python\n",
    "# Create a Python list of feature names\n",
    "FeatureNames = [\"season\", \"month\", \"hour\", \"holiday\", \"weekday\", \"workingday\",\n",
    "    \"weather\", \"temp\", \"feelslike\", \"humidity\", \"windspeed\"]\n",
    "    ```\n",
    "```python\n",
    "# Select a subset, X, of the original DataFrame \n",
    "X = data[FeatureNames]\n",
    "```\n",
    "\n",
    "```python\n",
    "# visualization\n",
    "visualizer = Rank2D(algorithm=\"pearson\")\n",
    "visualizer.fit_transform(X)\n",
    "visualizer.poof()\n",
    "```\n",
    "```Python\n",
    "Y = data[\"riders\"]\n",
    "```\n",
    "```Python\n",
    "#Split X and y into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "```\n",
    "```Python\n",
    "# Linear Regression Model\n",
    "linreg = LinearRegression()\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "# Check the coeffiecient\n",
    "linreg.coef_\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "#Visulize the residuals of training and testing model\n",
    "visualizer = ResidualsPlot(linreg)\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof()                 # Draw/show/poof the data\n",
    "```\n",
    "Exercise 2\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "# Organize our data\n",
    "y_names = data['target_names']\n",
    "y = data['target']\n",
    "feature_names = data['feature_names']\n",
    "X = data['data']\n",
    "# Split our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "#fit Gaussian Naive model\n",
    "gnb = GaussianNB()\n",
    "cm = ConfusionMatrix(gnb, classes=y_names, label_encoder={0:'malignant', 1: 'benign'}  )\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "```\n",
    "\n",
    "Exercise 3\n",
    "```python\n",
    "#heirotical single method\n",
    "# Create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=2, affinity = 'euclidean', linkage = 'single')\n",
    "# save clusters for chart\n",
    "cluster_moon['cluster_single'] = hc.fit_predict(x)\n",
    "cluster_moon.head(n=10)\n",
    "# plot the output\n",
    "plt.scatter(cluster_moon['x'], cluster_moon['y'],c=cluster_moon['cluster_single'], cmap = 'rainbow')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv(r\"C:\\Users\\XZHU8\\Documents\\OIT related\\Workshop\\Python for Machine Learning\\bikeshare.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureNames = [\"season\", \"month\", \"hour\", \"holiday\", \"weekday\", \"workingday\",\n",
    "    \"weather\", \"temp\", \"feelslike\", \"humidity\", \"windspeed\"]\n",
    "X=data[FeatureNames]\n",
    "visualizer = Rank2D(algorithm=\"pearson\")\n",
    "visualizer.fit_transform(X)\n",
    "visualizer.poof()\n",
    "y = data[\"riders\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the packages\n",
    "import pandas as pd\n",
    "#import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from yellowbrick.features import Rank2D\n",
    "from yellowbrick.regressor import ResidualsPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "linreg = LinearRegression()\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "# Visualzie the training and fitting model and the residual histogram\n",
    "visualizer = ResidualsPlot(linreg)\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof()                 # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
