{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Machine learning\n",
    "\n",
    "The basic idea of any machine learning model is to have a large number of inputs and also supplied the output applicable for them. After analysing more and more data, it tries to figure out the relationship between input and output.\n",
    "\n",
    "Consider a very simple example when you have to decide whether to wear a jacket or not depending on the outside temperature. You have the data below and we call it training data. \n",
    "\n",
    "\n",
    "| Outside Temperature | Wear a Jacket |\n",
    "|---------------------|---------------|\n",
    "| 90°F                | No            |\n",
    "| 80°F                | No            |\n",
    "| 70°F                | No            |\n",
    "| 60°F                | Yes           |\n",
    "| 20°F                | Yes           |\n",
    "\n",
    "Somehow, we find out a connection between the input (temperature) and the output (decision to wear a jacket).\n",
    "So, if the temperature is 65°F, you would still wear a jacket although you were never told the outcome for that particular temperature.\n",
    "\n",
    "Now, let's move on to a slightly better problem which the computer will solve for us.\n",
    "Before we begin, we need to import the scikit-learn package, it provides easy to use functions and a lot of machine learning models. We will use it for today's workshop. \n",
    "\n",
    "```python\n",
    "# install scikit package\n",
    "conda install -c conda-forge scikit-image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(\"sklearn version is:\" + sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Training Set\n",
    "Here, X is the input and y is the output.\n",
    "\n",
    "| x1 | x2 | y  |\n",
    "|----|----|----|\n",
    "| 1  | 2  | 5  |\n",
    "| 4  | 5  | 14 |\n",
    "| 11 | 12 | 35 |\n",
    "| 21 | 22 | 65 |\n",
    "| 5  | 5  | 15 |\n",
    "\n",
    "Given the training set you could easily guess that the output (y) is (x1 + 2````*````x2 ).\n",
    "\n",
    "## How to Generate a Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import randint function from random package\n",
    "from random import randint\n",
    "\n",
    "# Create two empty list to storage training data\n",
    "TrainInput = list()\n",
    "TrainOutput = list()\n",
    "\n",
    "# Generate 100 random set of x1 and x2\n",
    "for i in range(100):\n",
    "    x1 = randint(0, 1000) # generate random integers between 0 and 1000. \n",
    "    x2 = randint(0, 1000)\n",
    "    y = x1 + (2*x2)\n",
    "    #append method is to add x1 and x2 to the Train list.\n",
    "    TrainInput.append([x1, x2]) \n",
    "    TrainOutput.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainInput[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainOutput[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Model: Linear Regression\n",
    "Working with linear regression model is simple. Create a model, train it and then use it:)\n",
    "\n",
    "### Train the Model\n",
    "We have generated the training data already, so create a linear regression model and pass it the training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "predictor=LinearRegression()\n",
    "predictor.fit(X=TrainInput, y=TrainOutput)\n",
    "coefficient=predictor.coef_\n",
    "print('Coefficient : {}.'.format(coefficient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data\n",
    "\n",
    "X = [[10, 20]]\n",
    "\n",
    "The outcome should be 10+2```*```20 =50. Let us see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = [[10, 20]]\n",
    "Outcome = predictor.predict(X=Xtest)\n",
    "print('Outcome: {}'.format(outcome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Linear Regression Example\n",
    "\n",
    "Sales (in thousands of units) for a particular product as a function of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media. Suppose that in our role as statistical consultants we are asked to suggest.\n",
    "1. We want to find a function that given input budgets for TV, radio and newspaper predicts the output sales.\n",
    "2. Which media contribute to sales?\n",
    "3. Visualize the relationship between the features and response.\n",
    "\n",
    "Reference: https://medium.com/simple-ai/linear-regression-intro-to-machine-learning-6-6e320dbdaf06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the packages\n",
    "import pandas as pd\n",
    "#import the adversting.csv data\n",
    "data =  pd.read_csv(r\"I:\\Classes\\OIT_Training\\Python for Machine Learning\\Advertising.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>147.042500</td>\n",
       "      <td>23.264000</td>\n",
       "      <td>30.554000</td>\n",
       "      <td>14.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>85.854236</td>\n",
       "      <td>14.846809</td>\n",
       "      <td>21.778621</td>\n",
       "      <td>5.217457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74.375000</td>\n",
       "      <td>9.975000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>10.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>149.750000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>12.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>218.825000</td>\n",
       "      <td>36.525000</td>\n",
       "      <td>45.100000</td>\n",
       "      <td>17.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>296.400000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TV       radio   newspaper       sales\n",
       "count  200.000000  200.000000  200.000000  200.000000\n",
       "mean   147.042500   23.264000   30.554000   14.022500\n",
       "std     85.854236   14.846809   21.778621    5.217457\n",
       "min      0.700000    0.000000    0.300000    1.600000\n",
       "25%     74.375000    9.975000   12.750000   10.375000\n",
       "50%    149.750000   22.900000   25.750000   12.900000\n",
       "75%    218.825000   36.525000   45.100000   17.400000\n",
       "max    296.400000   49.600000  114.000000   27.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the first 6 rows of the data\n",
    "data.head()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python list of feature names\n",
    "feature_names = ['TV', 'radio', 'newspaper']\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the list to select a subset of the original DataFrame\n",
    "X = data[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the list to select a subeset of the original DataFrame\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize relationship between the variables\n",
    "from yellowbrick.features import Rank2D\n",
    "visualizer = Rank2D(algorithm=\"pearson\")\n",
    "visualizer.fit_transform(data)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split X and y into traning and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) # random_state=int, random_state is the seed used by the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Linear Regression Model\n",
    "linreg = LinearRegression()\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "# Check the coefficient\n",
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the regression line use seaborn \n",
    "# compute the R square value\n",
    "from sklearn.metrics import r2_score\n",
    "print('R2 Score:', r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "# Visualzie the training and fitting model and the residual histogram\n",
    "visualizer = ResidualsPlot(linreg)\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof()                 # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1:\n",
    "1. Import the packages using the python code below:\n",
    "```Python\n",
    "#import the packages\n",
    "import pandas as pd\n",
    "#import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from yellowbrick.features import Rank2D\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "```\n",
    "2. Import the data named \"'bikeshare.csv\" as a dataframe.\n",
    "```python\n",
    "data =  pd.read_csv(r\"C:\\Users\\XZHU8\\Documents\\OIT related\\Workshop\\Python for Machine Learning\\bikeshare.csv\")\n",
    "```\n",
    "3. Create a Python list of feature names:\n",
    "    1. X is \"season\", \"month\", \"hour\", \"holiday\", \"weekday\", \"workingday\",\n",
    "    \"weather\", \"temp\", \"feelslike\", \"humidity\", \"windspeed\".\n",
    "4. Use the list to select a subset, X, of the original DataFrame \n",
    "\n",
    "5. use the python syntax below to show the pearson correlaion matrix\n",
    "```python\n",
    "visualizer = Rank2D(algorithm=\"pearson\")\n",
    "visualizer.fit_transform(X)\n",
    "visualizer.poof()\n",
    "```\n",
    "6. Use the list to select a subset, y, of the original DataFrame \n",
    "    1. Y is \"riders\"\n",
    "7. Split X and y into training and testing sets  \n",
    "8. Create a linear regression function and fit the model from the training data, and test using the testing data.\n",
    "9. Check the coeffiecient\n",
    "10. Visulize the residuals of training and testing model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Classfication Problem\n",
    "\n",
    "The fruits dataset was created by Dr. Iain Murray from University of Edinburgh. He bought a few dozen oranges, lemons and apples of different varieties, and recorded their measurements in a table. And then the professors at University of Michigan formatted the fruits data slightly. Let us import the data and see the first several rows of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruit_label</th>\n",
       "      <th>fruit_name</th>\n",
       "      <th>fruit_subtype</th>\n",
       "      <th>mass</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>color_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>192</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>180</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>176</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>86</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>84</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fruit_label fruit_name fruit_subtype  mass  width  height  color_score\n",
       "0            1      apple  granny_smith   192    8.4     7.3         0.55\n",
       "1            1      apple  granny_smith   180    8.0     6.8         0.59\n",
       "2            1      apple  granny_smith   176    7.4     7.2         0.60\n",
       "3            2   mandarin      mandarin    86    6.2     4.7         0.80\n",
       "4            2   mandarin      mandarin    84    6.0     4.6         0.79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fruits = pd.read_table(r\"I:\\Classes\\OIT_Training\\Python for Machine Learning\\fruit_data_with_colors.txt\")\n",
    "fruits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the dataset represents one piece of the fruit as represented by several features that are in the table’s columns.\n",
    "\n",
    "We have 59 pieces of fruits and 7 features in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fruits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four types of fruits in the dataset: Apple, mandarin, orange, and lemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits.groupby('fruit_name').size()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(fruits['fruit_name'], color='b', label='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=\"width\", y=\"height\",hue=\"fruit_name\", style=\"fruit_name\", data=fruits)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statstics \n",
    "fruits.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram for each numeric imput variable\n",
    "from scipy.stats import norm\n",
    "fruits.drop('fruit_label' ,axis=1).hist(bins=30, figsize=(9,9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some numberical values do not have the same scale, so we need to scale them. But we will split the dataset into training and test sets first. Then we scale the training data and then apply scaling to the test set, because in practice you are not provided with test data and you just have to evaluate your model on test data.\n",
    "\n",
    "Here we use MinMaxScaler, which rescales the data set such that all feature values are in the range [0, 1]. \n",
    "\n",
    "The transformation formula is given by:\n",
    "\n",
    "$ z_i = \\frac{x_i - min(x_i)}{max(x_i) - min(x_i)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a feature list and y\n",
    "feature_names = ['mass', 'width', 'height', 'color_score']\n",
    "X = fruits[feature_names]\n",
    "y = fruits['fruit_label']\n",
    "#split the dataset into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "#Use minmax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a logistic regression model\n",
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
    "Reference: https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "logreg = LogisticRegression()\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(logreg, classes=[1,2,3,4])\n",
    "#cm = ConfusionMatrix(logreg, classes=[1,2,3,4], label_encoder={1: 'apple', 2: 'mandarin', 3: 'orange', 4:'lemon'} )\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(logreg.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree \n",
    "A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n",
    "Reference:https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dct = DecisionTreeClassifier()\n",
    "cm = ConfusionMatrix(dct, classes=[1,2,3,4])\n",
    "#cm = ConfusionMatrix(logreg, classes=[1,2,3,4], label_encoder={1: 'apple', 2: 'mandarin', 3: 'orange', 4:'lemon'} )\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(dct.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(dct.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "The KNN algorithm assumes that simliar things exist in close proximity. In other words, similar things are near to each other.\n",
    "Reference:https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "cm = ConfusionMatrix(knn, classes=[1,2,3,4])\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "Linear Discriminant Analysis is a dimensionality reduction technique used as a preprocessing step in Machine Learning and pattern classification applications.\n",
    "The main goal of dimensionality reduction techinques is to reduce the dimensions by removing the reduntant and dependent features by transforming the features from higher dimensional space to a space with lower dimensions.\n",
    "Reference:https://medium.com/@srishtisawla/linear-discriminant-analysis-d38decf48105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "cm = ConfusionMatrix(lda, classes=[1,2,3,4])\n",
    "#cm = ConfusionMatrix(logreg, classes=[1,2,3,4], label_encoder={1: 'apple', 2: 'mandarin', 3: 'orange', 4:'lemon'} )\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "\n",
    "print('Accuracy of LDA classifier on training set: {:.2f}'\n",
    "     .format(lda.score(X_train, y_train)))\n",
    "print('Accuracy of LDA classifier on test set: {:.2f}'\n",
    "     .format(lda.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "### Pros:\n",
    "\n",
    "It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n",
    "When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
    "It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
    "\n",
    "### Cons:\n",
    "\n",
    "If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\n",
    "On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "cm = ConfusionMatrix(gnb, classes=[1,2,3,4])\n",
    "#cm = ConfusionMatrix(logreg, classes=[1,2,3,4], label_encoder={1: 'apple', 2: 'mandarin', 3: 'orange', 4:'lemon'} )\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "print('Accuracy of GNB classifier on training set: {:.2f}'\n",
    "     .format(gnb.score(X_train, y_train)))\n",
    "print('Accuracy of GNB classifier on test set: {:.2f}'\n",
    "     .format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "Keywords: bootstrap aggregation or bagging\n",
    "          ensemble model, voting\n",
    "\n",
    "### RF Algorithm\n",
    "\n",
    "Fits a bunch of trees on \"random samples\n",
    "from our sample\" (called bootstrap samples)\n",
    "& they all vote on best class. The votes \n",
    "aggregated to choose the winning class.\n",
    "\n",
    "The concept of combining predictions \n",
    "like this is called \"bagging\" or \n",
    "Bootstrap AGgregation.\n",
    "\n",
    "Details:\n",
    "\n",
    "1. Randomly select (with replacement) both\n",
    "   N observations and a subset of predictors\n",
    "   to create 100-500 subsets of data.\n",
    "\n",
    "2. Fit a \"bushy\" tree to each sample\n",
    "   e.g. no pruning so each WILL overfit!\n",
    "\n",
    "3. At each split, variables are randomly sampled\n",
    "\n",
    "4. Have each model make a prediction, then \n",
    "   count (when classifying) or average \n",
    "   (when regressing) their predictions\n",
    "   (weights may be used based on model accuracy)\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "cm = ConfusionMatrix(rfc, classes=[1,2,3,4])\n",
    "#cm = ConfusionMatrix(logreg, classes=[1,2,3,4], label_encoder={1: 'apple', 2: 'mandarin', 3: 'orange', 4:'lemon'} )\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "print('Accuracy of RFC classifier on training set: {:.2f}'\n",
    "     .format(rfc.score(X_train, y_train)))\n",
    "print('Accuracy of RFC classifier on test set: {:.2f}'\n",
    "     .format(rfc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks (NN)\n",
    "\n",
    "### Neural Network Algorithm\n",
    "\n",
    "* Simulates human brain by weighting\n",
    "  \"neurons\" stored in \"hidden layers\"\n",
    "  \n",
    "* Each training observation adjusts\n",
    "  the impact of each neuron, often\n",
    "  through \"back-propogation\"\n",
    "\n",
    "* Deep Learning uses many layers\n",
    "  and many neurons per layer\n",
    "  \n",
    "\n",
    "### Neural Network Advantages\n",
    "  \n",
    "* Excellent results for extreme \n",
    "  complexity e.g. voice, \n",
    "  image recognition\n",
    "  \n",
    "* Though the model consists of a set\n",
    "  of equations that is fairly small\n",
    "  compared to many other metods\n",
    "  \n",
    "* Model is quick to apply to new data\n",
    "\n",
    "    \n",
    "### Neural Network Disadvantages\n",
    "\n",
    "* Computationally intensive\n",
    "  to train the model\n",
    "    \n",
    "* Performance on numerical data\n",
    "  rarely much better than faster\n",
    "  methods e.g. rf, gbm\n",
    "    \n",
    "* Models are impossible to interpet\n",
    "\n",
    "* Extremely sensitive to multicollinearity\n",
    "  (use PCA first or method = \"pcaNNet\")\n",
    "\n",
    "\n",
    "### Neural Network Tuning Parameters\n",
    "\n",
    "* Depends on specific type but\n",
    "  in general:\n",
    "  \n",
    "* Number of hidden layers\n",
    "\n",
    "* Number of neurons per layer\n",
    "\n",
    "* Type of feedback mechanism\n",
    "  e.g. back-propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001)\n",
    "\n",
    "cm = ConfusionMatrix(mlp, classes=[1,2,3,4])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "print('Accuracy of MLP classifier on training set: {:.2f}'\n",
    "     .format(mlp.score(X_train, y_train)))\n",
    "print('Accuracy of MLP classifier on test set: {:.2f}'\n",
    "     .format(mlp.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Common metrics for evaluating classifiers:\n",
    "\n",
    "Precision is the number of correct positive results divided by the number of all positive results (e.g. How many of the mushrooms we predicted would be edible actually were?).\n",
    "\n",
    "Recall is the number of correct positive results divided by the number of positive results that should have been returned (e.g. How many of the mushrooms that were poisonous did we accurately predict were poisonous?).\n",
    "\n",
    "The F1 score is a measure of a test’s accuracy. It considers both the precision and the recall of the test to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.\n",
    "``` python\n",
    "precision = true positives / (true positives + false positives)\n",
    "\n",
    "recall = true positives / (false negatives + true positives)\n",
    "\n",
    "F1 score = 2 * ((precision * recall) / (precision + recall))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassificationReport(knn, classes=[1,2,3,4], support=True)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data\n",
    "fruits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted vs Actual in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new=knn.predict(X_test)\n",
    "test = list(zip(y_new, y_test))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise2: \n",
    "\n",
    "The dataset we will be working with in this tutorial is the Breast Cancer Wisconsin Diagnostic Database. The dataset includes various information about breast cancer tumors, as well as classification labels of malignant or benign. The dataset has 569 instances, or data, on 569 tumors and includes information on 30 attributes, or features, such as the radius of the tumor, texture, smoothness, and area.\n",
    "\n",
    "Using this dataset, we will build a machine learning model to use tumor information to predict whether or not a tumor is malignant or benign.\n",
    "\n",
    "The syntax below imported and loaded the dataset, splitted the data into training and test and also applied Gaussian Naive Bayes model to the data.\n",
    "You need to fit the Logistic Regression and Random Forest model and find the best one. \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "# Organize our data\n",
    "y_names = data['target_names']\n",
    "y = data['target']\n",
    "feature_names = data['feature_names']\n",
    "X = data['data']\n",
    "# Split our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "#fit Gaussian Naive model\n",
    "gnb = GaussianNB()\n",
    "cm = ConfusionMatrix(gnb, classes=y_names, label_encoder={0:'malignant', 1: 'benign'}  )\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "print('Accuracy of RFC classifier on training set: {:.2f}'\n",
    "     .format(gnb.score(X_train, y_train)))\n",
    "print('Accuracy of RFC classifier on test set: {:.2f}'\n",
    "     .format(gnb.score(X_test, y_test)))\n",
    "     \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "# Organize our data\n",
    "y_names = data['target_names']\n",
    "y = data['target']\n",
    "feature_names = data['feature_names']\n",
    "X = data['data']\n",
    "\n",
    "\n",
    "# Split our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "\n",
    "#fit Gaussian Naive model\n",
    "gnb = GaussianNB()\n",
    "cm = ConfusionMatrix(gnb, classes=y_names, label_encoder={0:'malignant', 1: 'benign'}  )\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "print('Accuracy of RFC classifier on training set: {:.2f}'\n",
    "     .format(gnb.score(X_train, y_train)))\n",
    "print('Accuracy of RFC classifier on test set: {:.2f}'\n",
    "     .format(gnb.score(X_test, y_test)))\n",
    "\n",
    "# logistic regression\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "So far, we have only explored supervised Machine Learning algorithms and techniques to develop models where the data had labels previously known. In other words, our data had some target variables with specific values that we used to train our models.\n",
    "However, when dealing with real-world problems, most of the time, data will not come with predefined labels, so we will want to develop machine learning models that can classify correctly this data, by finding by themselves some commonality in the features, that will be used to predict the classes on new data.\n",
    "\n",
    "## Unsupervised Learning Analysis Process\n",
    "\n",
    "Unsupervised learning main applications are:\n",
    "* Segmenting datasets by some shared atributes.\n",
    "* Detecting anomalies that do not fit to any group.\n",
    "* Simplify datasets by aggregating variables with similar atributes.\n",
    "\n",
    "In summary, the main goal is to study the intrinsic (and commonly hidden) structure of the data.\n",
    "This techniques can be condensed in two main types of problems that unsupervised learning tries to solve. This problems are:\n",
    "\n",
    "* Clustering\n",
    "* Dimensionality Reduction\n",
    "\n",
    "This workshop we will cover the clustering problems. \n",
    "\n",
    "## Clutering Analysis\n",
    "\n",
    "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).\n",
    "![image.png](Unsupervise3.png)\n",
    "\n",
    "Clustering, however, has many different names (with respect to the fields it is being applied):\n",
    "\n",
    "* Cluster analysis\n",
    "* Automatic classification\n",
    "* Data segmentation\n",
    "\n",
    "### All the above names essentially mean clustering.\n",
    "\n",
    "Cluster analysis have an incredible wide range of applications and are quite useful to solve real world problems such as anomaly detection, recommending systems, documents grouping, or finding customers with common interests based on their purchases.\n",
    "Some of the most common clustering algorithms, and the ones that will be explored in the workshop, are:\n",
    "* K-Means\n",
    "* Hierarchichal Clustering\n",
    "* Density Based Scan Clustering (DBSCAN)\n",
    "* Gaussian Clustering Model\n",
    "\n",
    "## Choosing a Problem\n",
    "\n",
    "we will take an example of market segmentation. There will be certain features due to which the market is segmented. We will try to analyse the the type of customers in the market based on the features. The data set consist of 30 samples and features are satisfaction and loyalty respectively. \n",
    "\n",
    "## K-means Cluster\n",
    "\n",
    "k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster (Definition from Wiki).\n",
    "\n",
    "### How the K-means algorithm works\n",
    "To process the learning data, the K-means algorithm in data mining starts with a first group of randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative (repetitive) calculations to optimize the positions of the centroids\n",
    "It halts creating and optimizing clusters when either:\n",
    "The centroids have stabilized — there is no change in their values because the clustering has been successful.\n",
    "The defined number of iterations has been achieved.\n",
    "\n",
    "reference: https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Satisfaction</th>\n",
       "      <th>Loyalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Satisfaction  Loyalty\n",
       "0             4    -1.33\n",
       "1             6    -0.28\n",
       "2             5    -0.99\n",
       "3             7    -0.29\n",
       "4             4     1.06"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.cluster import KMeans\n",
    "# import data\n",
    "data=pd.read_csv(r\"I:\\Classes\\OIT_Training\\Python for Machine Learning\\kmeans clustering.csv\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "sns.scatterplot(x=\"Satisfaction\", y=\"Loyalty\",  data=data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy the data and ignore the feature name and store the data into a variable X.\n",
    "x = data.copy()\n",
    "\n",
    "# create a variable kmeans using kmeans function and passing the argument 2 in the Kmeans\n",
    "kmeans = KMeans(2)\n",
    "kmeans.fit(x)\n",
    "\n",
    "# Clustering result\n",
    "clusters = x.copy()\n",
    "clusters['cluster_pred'] = kmeans.fit_predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(clusters['Satisfaction'], clusters['Loyalty'],c=clusters['cluster_pred'], cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalthy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "The biggest problem here is that Satisfaction is choosen as a feature and loyalty has been neglected. We can see in the figure that all the element to the right of 6 forms one cluster and the other on the left forms another. This is a bias result because our algorithm has discarded the Loyalty feature. It has done the clustering only on the basis of satisfaction. This does not give an appropriate result through which we can analyze things.\n",
    "Satisfaction was choosen as the feature because it had large values.\n",
    "So here is the problem both the data are not scaled. First we have to standardize the data, so that both the data have equal weights in our clustering.\n",
    "We can’t neglect loyalty as it has an important role in the analyses of market segmentation.\n",
    "\n",
    "We will not go in depth of this as sklearn helps us to scale the data.\n",
    "The data is scaled around zero mean. Now we can see that both the data are equally scaled and now both will have equal chance of being selected as feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "x_scaled = preprocessing.scale(x)\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Elbow Method:\n",
    "Have we ever wondered why we initialized kmeans with 2 clusters only.\n",
    "Yes, we could have initialized it with any value we wanted we could have got any number of clusters. But the analyses becomes difficult when there are a large number of clusters. So how we will know the exact number of cluster to start off. Note there are no such exact number as it changes with the problem in hand.\n",
    "Here the elbow method comes handy when we are confused as to how may clusters do we need. What elbow method does is it starts of with making one cluster to the number of clusters in our sample and with the kmeans inertia value we determine what would be the appropriate number of clusters.\n",
    "Remember our goal- Our final goal was to minimize the within the cluster sum of square and maximize the distance between clusters.\n",
    "With this simple line of code we get all the inertia value or the within the cluster sum of square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range (1, 30):\n",
    "    kmeans = KMeans(i)\n",
    "    kmeans.fit(x_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "# visualized the Elbow method\n",
    "plt.plot(range(1,30), wcss)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph looks like elbow and we have to determine that elbow point.\n",
    "Here the elbow point comes at around 4 and this our optimal number of clusters for the above data which we should choose.\n",
    "If we look at the figure carefully after 4 when we go on increasing the number of cluster there is no big change in the wcss and it remains constant.\n",
    "Hurrah..!! we have got the optimal number of clusters for our problem.\n",
    "We will now quickly perform the kmeans clustering with the new number of clusters which is 4 and then dive into some analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_new = KMeans(4)\n",
    "kmeans.fit(x_scaled)\n",
    "cluster_new = x.copy()\n",
    "cluster_new['cluster_pred']=kmeans_new.fit_predict(x_scaled)\n",
    "cluster_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the newly cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(clusters['Satisfaction'], clusters['Loyalty'],c=cluster_new['cluster_pred'], cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalthy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "### How Hierarchical Clustering works\n",
    "Hierarchical clustering starts by treating each observation as a separate cluster. Then, it repeatedly executes the following two steps: (1) identify the two clusters that are closest together, and (2) merge the two most similar clusters. This continues until all the clusters are merged together. This is illustrated in the diagrams below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import hierarchical clustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrogram\n",
    "A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters.\n",
    "\n",
    "The key to interpreting a dendrogram is to focus on the height at which any two objects are joined together. In the example below, we can see that 4 and 22 are most similar, as the height of the link that joins them together is the smallest.\n",
    "\n",
    "Observations are allocated to clusters by drawing a horizontal line through the dendrogram. Observations that are joined together below the line are in clusters.\n",
    "\n",
    "Reference: https://www.displayr.com/what-is-dendrogram/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dendrogram \n",
    "from scipy.cluster import hierarchy\n",
    "Z = hierarchy.linkage(x_scaled,'ward')\n",
    "dn = hierarchy.dendrogram(Z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dendrogram\n",
    "dendrogram = sch.dendrogram(sch.linkage(x_scaled, method='ward'))\n",
    "\n",
    "# Create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage = 'ward')\n",
    "\n",
    "# save clusters for chart\n",
    "cluster_hc = x.copy()\n",
    "cluster_hc['cluster_pred'] = hc.fit_predict(x_scaled)\n",
    "cluster_hc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cluster\n",
    "plt.close()\n",
    "plt.scatter(cluster_hc['Satisfaction'], cluster_hc['Loyalty'],c=cluster_hc['cluster_pred'], cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalthy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Based Scan Clustering (DBSCAN)\n",
    "\n",
    "The key idea is that for each point of a cluster, the neighborhood of a given radius has to contain at least a minimum number of points. Compared to centroid-based clustering like K-Means, density-based clustering works by identifying “dense” clusters of points, allowing it to learn clusters of arbitrary shape and identify outliers in the data. \n",
    "\n",
    "**DBSCAN algorithm requires two parameters –**\n",
    "\n",
    "**eps** : It defines the neighborhood around a data point i.e. if the distance between two points is lower or equal to ‘eps’ then they are considered as neighbors. If the eps value is chosen too small then large part of the data will be considered as outliers. If it is chosen very large then the clusters will merge and majority of the data points will be in the same clusters. One way to find the eps value is based on the k-distance graph.\n",
    "\n",
    "**MinPts**: Minimum number of neighbors (data points) within eps radius. Larger the dataset, the larger value of MinPts must be chosen. As a general rule, the minimum MinPts can be derived from the number of dimensions D in the dataset as, MinPts >= D+1. The minimum value of MinPts must be chosen at least 3.\n",
    "In this algorithm, we have 3 types of data points.\n",
    "\n",
    "**Core Point**: A point is a core point if it has more than MinPts points within eps.\n",
    "**Border Point**: A point which has fewer than MinPts within eps but it is in the neighborhood of a core point.\n",
    "**Noise or outlier**: A point which is not a core point or border point.\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "db_default = DBSCAN(eps = 0.55, min_samples = 3).fit(x_scaled) \n",
    "labels = db_default.labels_ \n",
    "labels\n",
    "# save clusters for chart\n",
    "cluster_db = x.copy()\n",
    "cluster_db['cluster_pred'] =db_default.fit_predict(x_scaled)\n",
    "cluster_db\n",
    "\n",
    "# plot the cluster\n",
    "plt.close()\n",
    "plt.scatter(cluster_hc['Satisfaction'], cluster_hc['Loyalty'],c=cluster_db['cluster_pred'], cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalthy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.\n",
    "\n",
    "The GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data.\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/mixture.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=4).fit(x_scaled)\n",
    "labels = gmm.predict(x_scaled)\n",
    "labels\n",
    "# plot the cluter\n",
    "plt.close()\n",
    "plt.scatter(cluster_hc['Satisfaction'], cluster_hc['Loyalty'],c=labels, cmap = 'rainbow')\n",
    "plt.xlabel('Satisfaction')\n",
    "plt.ylabel('Loyalthy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis (The final step):\n",
    "Through the given figure following things can be interpreted:\n",
    "\n",
    "The sky blue dots are the people who are less satisfied and less loyal and therefore can be termed as alienated.\n",
    "The yellow dots are people with high loyalty and less satisfaction.\n",
    "The purple dots are the people with high loyalty and high satisfaction and they are the fans.\n",
    "The red dots are the people who are in the midst of things.\n",
    "The ultimate goal of any businessman would be to have as many people up there in the fans category. We are ready with a solution and we can target the audience as per our analysis. For example, the crowd who are supporters can easily be turned into fans by fulfilling their satisfaction level.\n",
    "\n",
    "reference:\n",
    "1. https://medium.com/code-to-express/k-means-clustering-for-beginners-using-python-from-scratch-f20e79c8ad00\n",
    "2. https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n",
    "3. https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html#spectral-clustering\n",
    "\n",
    "## Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "np.random.seed(0)\n",
    "# generate 2d classification dataset\n",
    "X,y= make_moons(n_samples=1000, noise=0.05)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmean method\n",
    "x = df[['x','y']]\n",
    "kmeans = KMeans(2)\n",
    "kmeans.fit(x)\n",
    "cluster_moon = df.copy()\n",
    "cluster_moon['cluster_kmean']=kmeans.fit_predict(x)\n",
    "cluster_moon.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the output\n",
    "plt.scatter(cluster_moon['x'], cluster_moon['y'],c=cluster_moon['cluster_kmean'], cmap = 'rainbow')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heirotical ward method\n",
    "\n",
    "# Create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=2, affinity = 'euclidean', linkage = 'ward')\n",
    "\n",
    "# save clusters for chart\n",
    "cluster_moon['cluster_ward'] = hc.fit_predict(x)\n",
    "cluster_moon.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the output\n",
    "plt.scatter(cluster_moon['x'], cluster_moon['y'],c=cluster_moon['cluster_ward'], cmap = 'rainbow')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Used the data above and try the single linkage clustering method and plot the data.\n",
    "\n",
    "```python\n",
    "#heirotical single method\n",
    "# Create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=2, affinity = 'euclidean', linkage = 'single')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "Exercise 1\n",
    "1. Import the packages using the python code below:\n",
    "```Python\n",
    "#import the packages\n",
    "import pandas as pd\n",
    "#import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from yellowbrick.features import Rank2D\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "```\n",
    "2. Import the data named \"'bikeshare.csv\" as a dataframe.\n",
    "```python\n",
    "data =  pd.read_csv(r\"C:\\Users\\XZHU8\\Documents\\OIT related\\Workshop\\Python for Machine Learning\\bikeshare.csv\")\n",
    "```\n",
    "3. Create a Python list of feature names:\n",
    "    X is \"season\", \"month\", \"hour\", \"holiday\", \"weekday\", \"workingday\",\n",
    "    \"weather\", \"temp\", \"feelslike\", \"humidity\", \"windspeed\".\n",
    "    \n",
    "4. Use the list to select a subset, X, of the original DataFrame \n",
    "\n",
    "5. Use the python syntax below to show the pearson correlaion matrix\n",
    "\n",
    "6. Use the list to select a subset, y, of the original DataFrame: \n",
    "    Y is \"riders\"\n",
    "\n",
    "7. Split X and y into training and testing sets \n",
    "\n",
    "8. Create a linear regression function and fit the model from the training data, and test using the testing data.\n",
    "\n",
    "9. Check the coeffiecient.\n",
    "\n",
    "10. Visulize the residuals of training and testing model \n",
    "\n",
    "```python\n",
    "# Create a Python list of feature names\n",
    "FeatureNames = [\"season\", \"month\", \"hour\", \"holiday\", \"weekday\", \"workingday\",\n",
    "    \"weather\", \"temp\", \"feelslike\", \"humidity\", \"windspeed\"]\n",
    "    ```\n",
    "```python\n",
    "# Select a subset, X, of the original DataFrame \n",
    "X = data[FeatureNames]\n",
    "```\n",
    "\n",
    "```python\n",
    "# visualization\n",
    "visualizer = Rank2D(algorithm=\"pearson\")\n",
    "visualizer.fit_transform(X)\n",
    "visualizer.poof()\n",
    "```\n",
    "```Python\n",
    "Y = data[\"riders\"]\n",
    "```\n",
    "```Python\n",
    "#Split X and y into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "```\n",
    "```Python\n",
    "# Linear Regression Model\n",
    "linreg = LinearRegression()\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "# Check the coeffiecient\n",
    "linreg.coef_\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "#Visulize the residuals of training and testing model\n",
    "visualizer = ResidualsPlot(linreg)\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof()                 # Draw/show/poof the data\n",
    "```\n",
    "Exercise 2\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "# Organize our data\n",
    "y_names = data['target_names']\n",
    "y = data['target']\n",
    "feature_names = data['feature_names']\n",
    "X = data['data']\n",
    "# Split our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "#fit Gaussian Naive model\n",
    "gnb = GaussianNB()\n",
    "cm = ConfusionMatrix(gnb, classes=y_names, label_encoder={0:'malignant', 1: 'benign'}  )\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "# How did we do?\n",
    "cm.poof()\n",
    "```\n",
    "\n",
    "Exercise 3\n",
    "```python\n",
    "#heirotical single method\n",
    "# Create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=2, affinity = 'euclidean', linkage = 'single')\n",
    "# save clusters for chart\n",
    "cluster_moon['cluster_single'] = hc.fit_predict(x)\n",
    "cluster_moon.head(n=10)\n",
    "# plot the output\n",
    "plt.scatter(cluster_moon['x'], cluster_moon['y'],c=cluster_moon['cluster_single'], cmap = 'rainbow')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data =  pd.read_csv(r\"C:\\Users\\XZHU8\\Documents\\OIT related\\Workshop\\Python for Machine Learning\\bikeshare.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureNames = [\"season\", \"month\", \"hour\", \"holiday\", \"weekday\", \"workingday\",\n",
    "    \"weather\", \"temp\", \"feelslike\", \"humidity\", \"windspeed\"]\n",
    "X=data[FeatureNames]\n",
    "visualizer = Rank2D(algorithm=\"pearson\")\n",
    "visualizer.fit_transform(X)\n",
    "visualizer.poof()\n",
    "y = data[\"riders\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the packages\n",
    "import pandas as pd\n",
    "#import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from yellowbrick.features import Rank2D\n",
    "from yellowbrick.regressor import ResidualsPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "linreg = LinearRegression()\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "# Visualzie the training and fitting model and the residual histogram\n",
    "visualizer = ResidualsPlot(linreg)\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof()                 # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
